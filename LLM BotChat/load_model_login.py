import os
import torch
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import snapshot_download, login


def download_and_load_4bit_model():
    """‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• gemma-3 ‡πÅ‡∏ö‡∏ö 4-bit"""

    load_dotenv()  # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .env
    hf_token = os.getenv('HF_TOKEN')
    if not hf_token:
        raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö HF_TOKEN ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô environment ‡∏´‡∏£‡∏∑‡∏≠ .env file")
    login(token=hf_token)

    if torch.cuda.is_available():
        print(f"‚úÖ GPU ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {torch.cuda.get_device_name(0)}")
        print(f"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU - ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 4-bit)")

    local_model_path = "./models/gemma-3"

    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...")
    print("‚ö†Ô∏è  ‡∏´‡∏≤‡∏Å error ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö access ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà:")
    print("   https://huggingface.co/google/gemma-3-1b-it")

    snapshot_download(
        repo_id="google/gemma-3-1b-it",
        local_dir=local_model_path,
        resume_download=True
    )
    print("‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à")

    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(local_model_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        print("‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ pad_token ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True, 
        bnb_4bit_compute_dtype=torch.float16, 
        bnb_4bit_quant_type="nf4", 
        bnb_4bit_use_double_quant=True, 
    )

    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö 4-bit...")
    print("‚ö†Ô∏è  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ...")

    model = AutoModelForCausalLM.from_pretrained(
        local_model_path,
        quantization_config=quantization_config,
        device_map="auto",  
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True 
    )

    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• 4-bit ‡πÄ‡∏™‡∏£‡πá‡∏à!")

    return local_model_path

if __name__ == "__main__":
    model_path = download_and_load_4bit_model()

    print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print(f"üìÅ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {model_path}")