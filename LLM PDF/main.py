import os
os.environ["TORCHDYNAMO_DISABLE"] = "1"
import fitz 
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch, glob, os

# -------------------- Config --------------------
EMB_MODEL = "intfloat/multilingual-e5-base"
MODEL_ID = "google/gemma-3-1b-it"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
PDF_PATH_PATTERN = "./docs/*.pdf"


# -------------------- Utils --------------------
def read_pdf(path):
    """‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
    pages = []
    with fitz.open(path) as doc:
        for i, page in enumerate(doc):
            pages.append(page.get_text("text").strip())
    return pages


def chunk_text(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks"""
    chunks = []
    start = 0
    while start < len(text):
        end = start + size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks


# -------------------- Embeddings --------------------
class SimpleIndex:
    def __init__(self, model_name=EMB_MODEL):
        self.embedder = SentenceTransformer(model_name)
        self.chunks = []
        self.embs = None

    def build(self, texts):
        self.chunks = texts
        self.embs = self.embedder.encode(
            ["passage: " + t for t in texts],
            normalize_embeddings=True
        )

    def search(self, query, top_k=3):
        q_emb = self.embedder.encode(["query: " + query], normalize_embeddings=True)
        scores = np.dot(self.embs, q_emb[0])
        idx = np.argsort(-scores)[:top_k]
        return [(self.chunks[i], scores[i]) for i in idx]


# -------------------- LLM --------------------
class SimpleLLM:
    def __init__(self, model_id=MODEL_ID):
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate(self, prompt, max_new_tokens=300):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            out = self.model.generate(**inputs, max_new_tokens=max_new_tokens)
        return self.tokenizer.decode(out[0], skip_special_tokens=True)


# -------------------- Pipeline --------------------
if __name__ == "__main__":
    # 1) ‡πÇ‡∏´‡∏•‡∏î PDF ‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå
    pdf_files = sorted(glob.glob(PDF_PATH_PATTERN))
    if not pdf_files:
        raise SystemExit("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå docs/")

    all_text = ""
    for path in pdf_files:
        print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô {os.path.basename(path)}")
        pages = read_pdf(path)
        all_text += "\n".join(pages) + "\n"

    # 2) ‡∏ï‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô chunks
    chunks = chunk_text(all_text)

    # 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ
    index = SimpleIndex()
    index.build(chunks)

    # 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM
    llm = SimpleLLM()

    # 5) ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    question = input("‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: ")

    results = index.search(question, top_k=3)
    context = "\n\n".join([r[0] for r in results])

    prompt = f"""
    ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    ‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
    {context}
    
    (‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢)
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
    """

    answer = llm.generate(prompt)
    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å
    answer = answer.strip().split("‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:")[1]

    print("\n===== SUMMARY =====\n")
    print(answer)
