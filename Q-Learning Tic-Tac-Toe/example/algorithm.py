import numpy as np
import pickle
import random
import os
from collections import defaultdict

class TicTacToeQLearning:
    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1, data_file="q_table.pkl"):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.data_file = data_file

        # ‡πÇ‡∏´‡∏•‡∏î Q-table ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        self.load_q_table()

    def save_q_table(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Q-table ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        with open(self.data_file, 'wb') as f:
            pickle.dump(dict(self.q_table), f)
        print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Q-table ‡πÅ‡∏•‡πâ‡∏ß ({len(self.q_table)} states)")

    def load_q_table(self):
        """‡πÇ‡∏´‡∏•‡∏î Q-table ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        if os.path.exists(self.data_file):
            with open(self.data_file, 'rb') as f:
                loaded_data = pickle.load(f)
                self.q_table = defaultdict(lambda: defaultdict(float), loaded_data)
            print(f"‡πÇ‡∏´‡∏•‡∏î Q-table ‡πÅ‡∏•‡πâ‡∏ß ({len(self.q_table)} states)")
        else:
            print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Q-table ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà")

    def state_to_string(self, board):
        """‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô key"""
        flat_board = board.flatten()
        converted = [str(2 if x == -1 else x) for x in flat_board]
        return ''.join(converted)

    def get_available_actions(self, board):
        """‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ö‡∏ô‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏ô"""
        return [i for i in range(9) if board.flatten()[i] == 0]

    def check_win_condition(self, board, player):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏ä‡∏ô‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏ñ‡∏ß
        for row in board:
            if np.sum(row == player) == 3:
                return True

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col in range(3):
            if np.sum(board[:, col] == player) == 3:
                return True

        # ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡πÅ‡∏¢‡∏á
        if np.sum([board[i, i] == player for i in range(3)]) == 3:
            return True
        if np.sum([board[i, 2 - i] == player for i in range(3)]) == 3:
            return True

        return False

    def get_action_reasoning(self, board, action, q_values, training=False):
        """‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action"""
        reasons = []

        # ‡πÅ‡∏™‡∏î‡∏á Q-value
        state = self.state_to_string(board)
        q_val = self.q_table[state][action - 1]
        if q_val > 0:
            reasons.append(f"üìà Q-value: {q_val:.3f} (‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å)")
        elif q_val < 0:
            reasons.append(f"üìâ Q-value: {q_val:.3f} (‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö)")
        else:
            reasons.append("üÜï Q-value: 0.000 (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏•‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏µ‡πâ)")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        if training and random.random() < self.epsilon:
            reasons.append(f"üé≤ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏£‡∏ß‡∏à (Œµ={self.epsilon})")

        return reasons

    def choose_action(self, board, training=True, show_thinking=False):
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ epsilon-greedy strategy"""
        state = self.state_to_string(board)
        available_actions = self.get_available_actions(board)

        if not available_actions:
            return None

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Q-values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å action ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        q_values = {}
        for action in available_actions:
            q_values[action] = self.q_table[state][action]

        if show_thinking:
            print("\nüß† AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...")
            print(f"üîç ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {state}")

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if state in self.q_table and any(self.q_table[state].values()):
                print("üìö AI ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô!")
            else:
                print("üÜï ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠")

            print("üìä Q-values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á:")

            # ‡πÅ‡∏™‡∏î‡∏á Q-values ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            for action in available_actions:
                q_val = q_values[action]
                if q_val > 0.001:
                    print(f"   ‡∏ä‡πà‡∏≠‡∏á {action + 1}: {q_val:.3f} üìà")
                elif q_val < -0.001:
                    print(f"   ‡∏ä‡πà‡∏≠‡∏á {action + 1}: {q_val:.3f} üìâ")
                else:
                    print(f"   ‡∏ä‡πà‡∏≠‡∏á {action + 1}: {q_val:.3f} ‚ö™")

        # ‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÉ‡∏ä‡πâ epsilon-greedy
        if training and random.random() < self.epsilon:
            action = random.choice(available_actions)
            selected_action = action + 1

            if show_thinking:
                print(f"\nüé≤ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏£‡∏ß‡∏à: ‡∏ä‡πà‡∏≠‡∏á {selected_action}")

            return selected_action

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡∏°‡∏µ Q-value ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        best_action = None
        best_value = -float('inf')

        for action in available_actions:
            q_value = q_values[action]
            if q_value > best_value:
                best_value = q_value
                best_action = action

        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ Q-value ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°
        if best_action is None:
            best_action = random.choice(available_actions)

        selected_action = best_action + 1

        if show_thinking:
            reasons = self.get_action_reasoning(board, selected_action, q_values, training)
            print(f"\n‚úÖ ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: ‡∏ä‡πà‡∏≠‡∏á {selected_action}")
            print("üí≠ ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•:")
            for reason in reasons:
                print(f"   {reason}")

        return selected_action

    def update_q_value(self, state, action, reward, next_state, show_learning=False):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó Q-value ‡∏î‡πâ‡∏ß‡∏¢ Q-learning formula"""
        old_q = self.q_table[state][action]

        # ‡∏´‡∏≤ max Q-value ‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        next_board_flat = [int(x) if x != '2' else -1 for x in next_state]
        next_board = np.array(next_board_flat).reshape(3, 3)
        next_available_actions = self.get_available_actions(next_board)

        if next_available_actions:
            max_next_q = max([self.q_table[next_state][a] for a in next_available_actions])
        else:
            max_next_q = 0

        # Q-learning formula
        new_q = old_q + self.learning_rate * (
                reward + self.discount_factor * max_next_q - old_q
        )

        self.q_table[state][action] = new_q

        if show_learning and abs(new_q - old_q) > 0.001:
            print(f"üìù ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ: ‡∏ä‡πà‡∏≠‡∏á {action + 1} | {old_q:.3f} ‚Üí {new_q:.3f} (Œî{new_q - old_q:+.3f})")
            if reward > 0:
                print(f"   ‚úÖ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• +{reward} ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ô‡∏µ‡πâ")
            elif reward < 0:
                print(f"   ‚ùå ‡πÇ‡∏î‡∏ô‡∏•‡∏á‡πÇ‡∏ó‡∏© {reward} ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ô‡∏µ‡πâ")
            else:
                print(f"   ‚öñÔ∏è6 ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ô‡∏µ‡πâ")

    def get_learning_stats(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"""
        total_states = len(self.q_table)
        total_actions = sum(len(actions) for actions in self.q_table.values())

        positive_q = 0
        negative_q = 0
        zero_q = 0

        for state_actions in self.q_table.values():
            for q_val in state_actions.values():
                if q_val > 0:
                    positive_q += 1
                elif q_val < 0:
                    negative_q += 1
                else:
                    zero_q += 1

        return {
            'total_states': total_states,
            'total_actions': total_actions,
            'positive_q': positive_q,
            'negative_q': negative_q,
            'zero_q': zero_q
        }


class TicTacToeGame:
    def __init__(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1

    def reset(self):
        """‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏Å‡∏°"""
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1

    def make_move(self, position, player):
        """‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô (position: 1-9)"""
        if position is None or position < 1 or position > 9:
            return False

        position = position - 1

        if self.board.flatten()[position] != 0:
            return False

        row, col = position // 3, position % 3
        self.board[row, col] = player
        return True

    def check_winner(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏π‡πâ‡∏ä‡∏ô‡∏∞"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏ñ‡∏ß
        for row in self.board:
            if abs(sum(row)) == 3:
                return row[0]

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col in range(3):
            if abs(sum(self.board[:, col])) == 3:
                return self.board[0, col]

        # ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡πÅ‡∏¢‡∏á
        if abs(sum([self.board[i, i] for i in range(3)])) == 3:
            return self.board[0, 0]

        if abs(sum([self.board[i, 2 - i] for i in range(3)])) == 3:
            return self.board[0, 2]

        # ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏™‡∏°‡∏≠
        if 0 not in self.board.flatten():
            return 0

        return None

    def print_board(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏ô"""
        symbols = {0: ' ', 1: 'X', -1: 'O'}
        RED = '\033[31m'
        GREEN = '\033[32m'
        BLUE = '\033[34m'
        RESET = '\033[0m'

        print("  " + "=" * 13)
        for i in range(3):
            row_str = "  |"
            for j in range(3):
                if self.board[i, j] == 0:
                    pos = i * 3 + j + 1
                    row_str += f" {GREEN}{pos}{RESET} |"
                elif self.board[i, j] == 1:
                    row_str += f" {RED}{symbols[self.board[i, j]]}{RESET} |"
                else:
                    row_str += f" {BLUE}{symbols[self.board[i, j]]}{RESET} |"
            print(row_str)
            if i < 2:
                print("  " + "-" * 13)
        print("  " + "=" * 13)